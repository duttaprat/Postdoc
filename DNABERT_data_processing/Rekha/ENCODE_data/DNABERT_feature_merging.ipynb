{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed2e230-6555-4da4-a88e-ad247a7763ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65f2c67-7c18-4647-a9a1-54a46639326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/data/projects/Enhancer/Finetuned_data/CPG2_KMer_all/cached_train_6-new-12w-0_210_dnaprom\"\n",
    "input_sequence_path = \"/data/projects/Enhancer/Finetuned_data/CPG2_KMer_all/sequence_train.tsv\" \n",
    "input_train_path = \"/data/projects/Enhancer/Finetuned_data/CPG2_KMer_all/train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "903c471e-4ae7-4eea-8709-05d57fea79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(input_path)\n",
    "sequence_data = pd.read_csv(input_sequence_path , sep=\"\\t\")\n",
    "train_data = pd.read_csv(input_train_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f105b753-26b0-4787-9eba-4528d98dc522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210022"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e34748-cc75-4620-ab2c-cd25e81d522e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210022, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ec2e00-a1dc-4643-ae7d-e1d1d0452170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210022, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774018b7-6b1c-4310-8c4a-418af89735f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in data], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in data], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322eb840-5ed1-42fb-9d34-9f1b09913879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 2671, 2480, 1713, 2744, 2770, 2873, 3288,  851, 3390, 1260,  932,\n",
       "        3716, 2561, 2039, 4046, 3881, 3223,  589, 2344, 1172,  579, 2304, 1011,\n",
       "        4031, 3823, 2992, 3763, 2751, 2797, 2983, 3727, 2605, 2215,  654, 2603,\n",
       "        2207,  623, 2477, 1704, 2707, 2622, 2281,  917, 3654, 2314, 1050,   90,\n",
       "         346, 1370, 1370, 1372, 1378, 1401, 1494, 1866, 3354, 1114,  346, 1370,\n",
       "        1369, 1368, 1362, 1337, 1240,  849, 3384, 1233,  822, 3276,  804, 3204,\n",
       "         516, 2050, 4090, 4058, 3931, 3421, 1383, 1423, 1581, 2214,  652, 2594,\n",
       "        2170,  473, 1880, 3411, 1343, 1261,  936, 3732, 2625, 2294,  972, 3876,\n",
       "        3202,  507, 2014, 3947, 3488, 1649, 2486, 1739, 2846, 3179,  415, 1646,\n",
       "        2476, 1697, 2679, 2511, 1838, 3243,  672, 2674, 2492, 1761, 2934, 3531,\n",
       "        1822, 3180,  419, 1663, 2542, 1964, 3747, 2687, 2542, 1963, 3741, 2664,\n",
       "        2451, 1599, 2286,  939, 3743, 2671, 2477, 1701, 2693, 2568, 2066,   60,\n",
       "         227,  894, 3564, 1956, 3716, 2561, 2038, 4042, 3865, 3159,  333, 1320,\n",
       "        1172,  579, 2304, 1010, 4028, 3809, 2936, 3539, 1855, 3309,  935, 3727,\n",
       "        2608, 2227,  704, 2803, 3007, 3823, 2992, 3764, 2755, 2815, 3055, 4014,\n",
       "        3753, 2709, 2630, 2313, 1047,   77,  296, 1171,  574, 2281,  918, 3658,\n",
       "        2330, 1114,  346, 1370,    3,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de2e26f7-b7cd-4893-b413-d363cad23a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1850,  0.3050,  0.2350,  0.2750, -0.1600, -0.0200, -0.0800,  0.2513,\n",
       "          0.3485,  0.3687]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(sequence_data[:1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5267c5d-2a2b-4f9e-b6c8-1dc4952e5bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ac1479b-fe5b-4404-b790-ef411fd2d8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(sequence_data.values).float().size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06b3f48-1514-4d0c-9855-0ee44a58c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_feature_length = len(sequence_data.columns)\n",
    "add_feature_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc525af-0e0a-4f59-986c-cddb443e067e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids[:, -(add_feature_length+4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d98c5f0-1ac0-43a2-a46c-8e90bc7c5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = torch.cat([all_input_ids[:, :-(add_feature_length+4)], torch.tensor(sequence_data.values), all_input_ids[:, -(add_feature_length+4):-add_feature_length]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a762ab-4f37-4f09-97cd-8aa3f6e5a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210022, 210])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0554eee4-dd28-4a9f-a9bc-69c3011093ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_mask[:, -(add_feature_length+4):-3]=1\n",
    "#all_attention_mask[:, -13:-3]\n",
    "all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c980eb46-d340-4993-99bc-2810b0538f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., dtype=torch.float64) tensor(1)\n",
      "tensor(2671., dtype=torch.float64) tensor(1)\n",
      "tensor(2480., dtype=torch.float64) tensor(1)\n",
      "tensor(1713., dtype=torch.float64) tensor(1)\n",
      "tensor(2744., dtype=torch.float64) tensor(1)\n",
      "tensor(2770., dtype=torch.float64) tensor(1)\n",
      "tensor(2873., dtype=torch.float64) tensor(1)\n",
      "tensor(3288., dtype=torch.float64) tensor(1)\n",
      "tensor(851., dtype=torch.float64) tensor(1)\n",
      "tensor(3390., dtype=torch.float64) tensor(1)\n",
      "tensor(1260., dtype=torch.float64) tensor(1)\n",
      "tensor(932., dtype=torch.float64) tensor(1)\n",
      "tensor(3716., dtype=torch.float64) tensor(1)\n",
      "tensor(2561., dtype=torch.float64) tensor(1)\n",
      "tensor(2039., dtype=torch.float64) tensor(1)\n",
      "tensor(4046., dtype=torch.float64) tensor(1)\n",
      "tensor(3881., dtype=torch.float64) tensor(1)\n",
      "tensor(3223., dtype=torch.float64) tensor(1)\n",
      "tensor(589., dtype=torch.float64) tensor(1)\n",
      "tensor(2344., dtype=torch.float64) tensor(1)\n",
      "tensor(1172., dtype=torch.float64) tensor(1)\n",
      "tensor(579., dtype=torch.float64) tensor(1)\n",
      "tensor(2304., dtype=torch.float64) tensor(1)\n",
      "tensor(1011., dtype=torch.float64) tensor(1)\n",
      "tensor(4031., dtype=torch.float64) tensor(1)\n",
      "tensor(3823., dtype=torch.float64) tensor(1)\n",
      "tensor(2992., dtype=torch.float64) tensor(1)\n",
      "tensor(3763., dtype=torch.float64) tensor(1)\n",
      "tensor(2751., dtype=torch.float64) tensor(1)\n",
      "tensor(2797., dtype=torch.float64) tensor(1)\n",
      "tensor(2983., dtype=torch.float64) tensor(1)\n",
      "tensor(3727., dtype=torch.float64) tensor(1)\n",
      "tensor(2605., dtype=torch.float64) tensor(1)\n",
      "tensor(2215., dtype=torch.float64) tensor(1)\n",
      "tensor(654., dtype=torch.float64) tensor(1)\n",
      "tensor(2603., dtype=torch.float64) tensor(1)\n",
      "tensor(2207., dtype=torch.float64) tensor(1)\n",
      "tensor(623., dtype=torch.float64) tensor(1)\n",
      "tensor(2477., dtype=torch.float64) tensor(1)\n",
      "tensor(1704., dtype=torch.float64) tensor(1)\n",
      "tensor(2707., dtype=torch.float64) tensor(1)\n",
      "tensor(2622., dtype=torch.float64) tensor(1)\n",
      "tensor(2281., dtype=torch.float64) tensor(1)\n",
      "tensor(917., dtype=torch.float64) tensor(1)\n",
      "tensor(3654., dtype=torch.float64) tensor(1)\n",
      "tensor(2314., dtype=torch.float64) tensor(1)\n",
      "tensor(1050., dtype=torch.float64) tensor(1)\n",
      "tensor(90., dtype=torch.float64) tensor(1)\n",
      "tensor(346., dtype=torch.float64) tensor(1)\n",
      "tensor(1370., dtype=torch.float64) tensor(1)\n",
      "tensor(1370., dtype=torch.float64) tensor(1)\n",
      "tensor(1372., dtype=torch.float64) tensor(1)\n",
      "tensor(1378., dtype=torch.float64) tensor(1)\n",
      "tensor(1401., dtype=torch.float64) tensor(1)\n",
      "tensor(1494., dtype=torch.float64) tensor(1)\n",
      "tensor(1866., dtype=torch.float64) tensor(1)\n",
      "tensor(3354., dtype=torch.float64) tensor(1)\n",
      "tensor(1114., dtype=torch.float64) tensor(1)\n",
      "tensor(346., dtype=torch.float64) tensor(1)\n",
      "tensor(1370., dtype=torch.float64) tensor(1)\n",
      "tensor(1369., dtype=torch.float64) tensor(1)\n",
      "tensor(1368., dtype=torch.float64) tensor(1)\n",
      "tensor(1362., dtype=torch.float64) tensor(1)\n",
      "tensor(1337., dtype=torch.float64) tensor(1)\n",
      "tensor(1240., dtype=torch.float64) tensor(1)\n",
      "tensor(849., dtype=torch.float64) tensor(1)\n",
      "tensor(3384., dtype=torch.float64) tensor(1)\n",
      "tensor(1233., dtype=torch.float64) tensor(1)\n",
      "tensor(822., dtype=torch.float64) tensor(1)\n",
      "tensor(3276., dtype=torch.float64) tensor(1)\n",
      "tensor(804., dtype=torch.float64) tensor(1)\n",
      "tensor(3204., dtype=torch.float64) tensor(1)\n",
      "tensor(516., dtype=torch.float64) tensor(1)\n",
      "tensor(2050., dtype=torch.float64) tensor(1)\n",
      "tensor(4090., dtype=torch.float64) tensor(1)\n",
      "tensor(4058., dtype=torch.float64) tensor(1)\n",
      "tensor(3931., dtype=torch.float64) tensor(1)\n",
      "tensor(3421., dtype=torch.float64) tensor(1)\n",
      "tensor(1383., dtype=torch.float64) tensor(1)\n",
      "tensor(1423., dtype=torch.float64) tensor(1)\n",
      "tensor(1581., dtype=torch.float64) tensor(1)\n",
      "tensor(2214., dtype=torch.float64) tensor(1)\n",
      "tensor(652., dtype=torch.float64) tensor(1)\n",
      "tensor(2594., dtype=torch.float64) tensor(1)\n",
      "tensor(2170., dtype=torch.float64) tensor(1)\n",
      "tensor(473., dtype=torch.float64) tensor(1)\n",
      "tensor(1880., dtype=torch.float64) tensor(1)\n",
      "tensor(3411., dtype=torch.float64) tensor(1)\n",
      "tensor(1343., dtype=torch.float64) tensor(1)\n",
      "tensor(1261., dtype=torch.float64) tensor(1)\n",
      "tensor(936., dtype=torch.float64) tensor(1)\n",
      "tensor(3732., dtype=torch.float64) tensor(1)\n",
      "tensor(2625., dtype=torch.float64) tensor(1)\n",
      "tensor(2294., dtype=torch.float64) tensor(1)\n",
      "tensor(972., dtype=torch.float64) tensor(1)\n",
      "tensor(3876., dtype=torch.float64) tensor(1)\n",
      "tensor(3202., dtype=torch.float64) tensor(1)\n",
      "tensor(507., dtype=torch.float64) tensor(1)\n",
      "tensor(2014., dtype=torch.float64) tensor(1)\n",
      "tensor(3947., dtype=torch.float64) tensor(1)\n",
      "tensor(3488., dtype=torch.float64) tensor(1)\n",
      "tensor(1649., dtype=torch.float64) tensor(1)\n",
      "tensor(2486., dtype=torch.float64) tensor(1)\n",
      "tensor(1739., dtype=torch.float64) tensor(1)\n",
      "tensor(2846., dtype=torch.float64) tensor(1)\n",
      "tensor(3179., dtype=torch.float64) tensor(1)\n",
      "tensor(415., dtype=torch.float64) tensor(1)\n",
      "tensor(1646., dtype=torch.float64) tensor(1)\n",
      "tensor(2476., dtype=torch.float64) tensor(1)\n",
      "tensor(1697., dtype=torch.float64) tensor(1)\n",
      "tensor(2679., dtype=torch.float64) tensor(1)\n",
      "tensor(2511., dtype=torch.float64) tensor(1)\n",
      "tensor(1838., dtype=torch.float64) tensor(1)\n",
      "tensor(3243., dtype=torch.float64) tensor(1)\n",
      "tensor(672., dtype=torch.float64) tensor(1)\n",
      "tensor(2674., dtype=torch.float64) tensor(1)\n",
      "tensor(2492., dtype=torch.float64) tensor(1)\n",
      "tensor(1761., dtype=torch.float64) tensor(1)\n",
      "tensor(2934., dtype=torch.float64) tensor(1)\n",
      "tensor(3531., dtype=torch.float64) tensor(1)\n",
      "tensor(1822., dtype=torch.float64) tensor(1)\n",
      "tensor(3180., dtype=torch.float64) tensor(1)\n",
      "tensor(419., dtype=torch.float64) tensor(1)\n",
      "tensor(1663., dtype=torch.float64) tensor(1)\n",
      "tensor(2542., dtype=torch.float64) tensor(1)\n",
      "tensor(1964., dtype=torch.float64) tensor(1)\n",
      "tensor(3747., dtype=torch.float64) tensor(1)\n",
      "tensor(2687., dtype=torch.float64) tensor(1)\n",
      "tensor(2542., dtype=torch.float64) tensor(1)\n",
      "tensor(1963., dtype=torch.float64) tensor(1)\n",
      "tensor(3741., dtype=torch.float64) tensor(1)\n",
      "tensor(2664., dtype=torch.float64) tensor(1)\n",
      "tensor(2451., dtype=torch.float64) tensor(1)\n",
      "tensor(1599., dtype=torch.float64) tensor(1)\n",
      "tensor(2286., dtype=torch.float64) tensor(1)\n",
      "tensor(939., dtype=torch.float64) tensor(1)\n",
      "tensor(3743., dtype=torch.float64) tensor(1)\n",
      "tensor(2671., dtype=torch.float64) tensor(1)\n",
      "tensor(2477., dtype=torch.float64) tensor(1)\n",
      "tensor(1701., dtype=torch.float64) tensor(1)\n",
      "tensor(2693., dtype=torch.float64) tensor(1)\n",
      "tensor(2568., dtype=torch.float64) tensor(1)\n",
      "tensor(2066., dtype=torch.float64) tensor(1)\n",
      "tensor(60., dtype=torch.float64) tensor(1)\n",
      "tensor(227., dtype=torch.float64) tensor(1)\n",
      "tensor(894., dtype=torch.float64) tensor(1)\n",
      "tensor(3564., dtype=torch.float64) tensor(1)\n",
      "tensor(1956., dtype=torch.float64) tensor(1)\n",
      "tensor(3716., dtype=torch.float64) tensor(1)\n",
      "tensor(2561., dtype=torch.float64) tensor(1)\n",
      "tensor(2038., dtype=torch.float64) tensor(1)\n",
      "tensor(4042., dtype=torch.float64) tensor(1)\n",
      "tensor(3865., dtype=torch.float64) tensor(1)\n",
      "tensor(3159., dtype=torch.float64) tensor(1)\n",
      "tensor(333., dtype=torch.float64) tensor(1)\n",
      "tensor(1320., dtype=torch.float64) tensor(1)\n",
      "tensor(1172., dtype=torch.float64) tensor(1)\n",
      "tensor(579., dtype=torch.float64) tensor(1)\n",
      "tensor(2304., dtype=torch.float64) tensor(1)\n",
      "tensor(1010., dtype=torch.float64) tensor(1)\n",
      "tensor(4028., dtype=torch.float64) tensor(1)\n",
      "tensor(3809., dtype=torch.float64) tensor(1)\n",
      "tensor(2936., dtype=torch.float64) tensor(1)\n",
      "tensor(3539., dtype=torch.float64) tensor(1)\n",
      "tensor(1855., dtype=torch.float64) tensor(1)\n",
      "tensor(3309., dtype=torch.float64) tensor(1)\n",
      "tensor(935., dtype=torch.float64) tensor(1)\n",
      "tensor(3727., dtype=torch.float64) tensor(1)\n",
      "tensor(2608., dtype=torch.float64) tensor(1)\n",
      "tensor(2227., dtype=torch.float64) tensor(1)\n",
      "tensor(704., dtype=torch.float64) tensor(1)\n",
      "tensor(2803., dtype=torch.float64) tensor(1)\n",
      "tensor(3007., dtype=torch.float64) tensor(1)\n",
      "tensor(3823., dtype=torch.float64) tensor(1)\n",
      "tensor(2992., dtype=torch.float64) tensor(1)\n",
      "tensor(3764., dtype=torch.float64) tensor(1)\n",
      "tensor(2755., dtype=torch.float64) tensor(1)\n",
      "tensor(2815., dtype=torch.float64) tensor(1)\n",
      "tensor(3055., dtype=torch.float64) tensor(1)\n",
      "tensor(4014., dtype=torch.float64) tensor(1)\n",
      "tensor(3753., dtype=torch.float64) tensor(1)\n",
      "tensor(2709., dtype=torch.float64) tensor(1)\n",
      "tensor(2630., dtype=torch.float64) tensor(1)\n",
      "tensor(2313., dtype=torch.float64) tensor(1)\n",
      "tensor(1047., dtype=torch.float64) tensor(1)\n",
      "tensor(77., dtype=torch.float64) tensor(1)\n",
      "tensor(296., dtype=torch.float64) tensor(1)\n",
      "tensor(1171., dtype=torch.float64) tensor(1)\n",
      "tensor(574., dtype=torch.float64) tensor(1)\n",
      "tensor(2281., dtype=torch.float64) tensor(1)\n",
      "tensor(918., dtype=torch.float64) tensor(1)\n",
      "tensor(3658., dtype=torch.float64) tensor(1)\n",
      "tensor(2330., dtype=torch.float64) tensor(1)\n",
      "tensor(1114., dtype=torch.float64) tensor(1)\n",
      "tensor(346., dtype=torch.float64) tensor(1)\n",
      "tensor(1370., dtype=torch.float64) tensor(1)\n",
      "tensor(0.1850, dtype=torch.float64) tensor(1)\n",
      "tensor(0.3050, dtype=torch.float64) tensor(1)\n",
      "tensor(0.2350, dtype=torch.float64) tensor(1)\n",
      "tensor(0.2750, dtype=torch.float64) tensor(1)\n",
      "tensor(-0.1600, dtype=torch.float64) tensor(1)\n",
      "tensor(-0.0200, dtype=torch.float64) tensor(1)\n",
      "tensor(-0.0800, dtype=torch.float64) tensor(1)\n",
      "tensor(0.2513, dtype=torch.float64) tensor(1)\n",
      "tensor(0.3485, dtype=torch.float64) tensor(1)\n",
      "tensor(0.3687, dtype=torch.float64) tensor(1)\n",
      "tensor(3., dtype=torch.float64) tensor(1)\n",
      "tensor(0., dtype=torch.float64) tensor(0)\n",
      "tensor(0., dtype=torch.float64) tensor(0)\n",
      "tensor(0., dtype=torch.float64) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(result1[0] , all_attention_mask[0]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e07b6ca-3757-443b-b0ca-a710181e54ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0000e+00,  2.6710e+03,  2.4800e+03,  1.7130e+03,  2.7440e+03,\n",
       "         2.7700e+03,  2.8730e+03,  3.2880e+03,  8.5100e+02,  3.3900e+03,\n",
       "         1.2600e+03,  9.3200e+02,  3.7160e+03,  2.5610e+03,  2.0390e+03,\n",
       "         4.0460e+03,  3.8810e+03,  3.2230e+03,  5.8900e+02,  2.3440e+03,\n",
       "         1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0110e+03,  4.0310e+03,\n",
       "         3.8230e+03,  2.9920e+03,  3.7630e+03,  2.7510e+03,  2.7970e+03,\n",
       "         2.9830e+03,  3.7270e+03,  2.6050e+03,  2.2150e+03,  6.5400e+02,\n",
       "         2.6030e+03,  2.2070e+03,  6.2300e+02,  2.4770e+03,  1.7040e+03,\n",
       "         2.7070e+03,  2.6220e+03,  2.2810e+03,  9.1700e+02,  3.6540e+03,\n",
       "         2.3140e+03,  1.0500e+03,  9.0000e+01,  3.4600e+02,  1.3700e+03,\n",
       "         1.3700e+03,  1.3720e+03,  1.3780e+03,  1.4010e+03,  1.4940e+03,\n",
       "         1.8660e+03,  3.3540e+03,  1.1140e+03,  3.4600e+02,  1.3700e+03,\n",
       "         1.3690e+03,  1.3680e+03,  1.3620e+03,  1.3370e+03,  1.2400e+03,\n",
       "         8.4900e+02,  3.3840e+03,  1.2330e+03,  8.2200e+02,  3.2760e+03,\n",
       "         8.0400e+02,  3.2040e+03,  5.1600e+02,  2.0500e+03,  4.0900e+03,\n",
       "         4.0580e+03,  3.9310e+03,  3.4210e+03,  1.3830e+03,  1.4230e+03,\n",
       "         1.5810e+03,  2.2140e+03,  6.5200e+02,  2.5940e+03,  2.1700e+03,\n",
       "         4.7300e+02,  1.8800e+03,  3.4110e+03,  1.3430e+03,  1.2610e+03,\n",
       "         9.3600e+02,  3.7320e+03,  2.6250e+03,  2.2940e+03,  9.7200e+02,\n",
       "         3.8760e+03,  3.2020e+03,  5.0700e+02,  2.0140e+03,  3.9470e+03,\n",
       "         3.4880e+03,  1.6490e+03,  2.4860e+03,  1.7390e+03,  2.8460e+03,\n",
       "         3.1790e+03,  4.1500e+02,  1.6460e+03,  2.4760e+03,  1.6970e+03,\n",
       "         2.6790e+03,  2.5110e+03,  1.8380e+03,  3.2430e+03,  6.7200e+02,\n",
       "         2.6740e+03,  2.4920e+03,  1.7610e+03,  2.9340e+03,  3.5310e+03,\n",
       "         1.8220e+03,  3.1800e+03,  4.1900e+02,  1.6630e+03,  2.5420e+03,\n",
       "         1.9640e+03,  3.7470e+03,  2.6870e+03,  2.5420e+03,  1.9630e+03,\n",
       "         3.7410e+03,  2.6640e+03,  2.4510e+03,  1.5990e+03,  2.2860e+03,\n",
       "         9.3900e+02,  3.7430e+03,  2.6710e+03,  2.4770e+03,  1.7010e+03,\n",
       "         2.6930e+03,  2.5680e+03,  2.0660e+03,  6.0000e+01,  2.2700e+02,\n",
       "         8.9400e+02,  3.5640e+03,  1.9560e+03,  3.7160e+03,  2.5610e+03,\n",
       "         2.0380e+03,  4.0420e+03,  3.8650e+03,  3.1590e+03,  3.3300e+02,\n",
       "         1.3200e+03,  1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0100e+03,\n",
       "         4.0280e+03,  3.8090e+03,  2.9360e+03,  3.5390e+03,  1.8550e+03,\n",
       "         3.3090e+03,  9.3500e+02,  3.7270e+03,  2.6080e+03,  2.2270e+03,\n",
       "         7.0400e+02,  2.8030e+03,  3.0070e+03,  3.8230e+03,  2.9920e+03,\n",
       "         3.7640e+03,  2.7550e+03,  2.8150e+03,  3.0550e+03,  4.0140e+03,\n",
       "         3.7530e+03,  2.7090e+03,  2.6300e+03,  2.3130e+03,  1.0470e+03,\n",
       "         7.7000e+01,  2.9600e+02,  1.1710e+03,  5.7400e+02,  2.2810e+03,\n",
       "         9.1800e+02,  3.6580e+03,  2.3300e+03,  1.1140e+03,  3.4600e+02,\n",
       "         1.3700e+03,  1.8500e-01,  3.0500e-01,  2.3500e-01,  2.7500e-01,\n",
       "        -1.6000e-01, -2.0000e-02, -8.0000e-02,  2.5126e-01,  3.4848e-01,\n",
       "         3.6869e-01,  3.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4471c4a4-0226-442a-b7b7-baab5237048c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    nan,  0.5088,  0.3688,  ...,     nan,     nan,     nan],\n",
       "        [    nan, -0.0645,  1.5947,  ...,     nan,     nan,     nan],\n",
       "        [    nan,  0.9964, -1.2049,  ...,     nan,     nan,     nan],\n",
       "        ...,\n",
       "        [    nan,  0.3837, -0.1362,  ...,     nan,     nan,     nan],\n",
       "        [    nan, -0.9463,  1.5748,  ...,     nan,     nan,     nan],\n",
       "        [    nan, -1.2411,  0.3878,  ...,     nan,     nan,     nan]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val = torch.mean(result1, dim=0, keepdim=True)\n",
    "std_val = torch.std(result1, dim=0, keepdim=True)\n",
    "\n",
    "normalized_tensor = (result1 - mean_val) / std_val\n",
    "normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1d6f61bc-613e-4bd4-8244-7462ca8fb7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000,  0.5088,  0.3688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.0000, -0.0645,  1.5947,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.0000,  0.9964, -1.2049,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 2.0000,  0.3837, -0.1362,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.0000, -0.9463,  1.5748,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.0000, -1.2411,  0.3878,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val = torch.mean(result1, dim=0, keepdim=True)\n",
    "std_val = torch.std(result1, dim=0, keepdim=True)\n",
    "\n",
    "# normalized_tensor = (result1 - mean_val) / std_val\n",
    "# normalized_tensor\n",
    "\n",
    "# Create a mask where standard deviation is zero\n",
    "mask_std_zero = std_val == 0\n",
    "\n",
    "# Normalize where std is not zero\n",
    "normalized_tensor = (result1 - mean_val) / torch.where(mask_std_zero, torch.ones_like(std_val), std_val)\n",
    "\n",
    "# Assign original values to columns with std=0\n",
    "normalized_tensor[:, mask_std_zero.squeeze()] = result1[:, mask_std_zero.squeeze()]\n",
    "normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67b66481-8219-4948-a83d-2f12b3c638da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7f110b6-b65b-4dca-9f42-32734a9f75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d42bd7f3-6e38-4140-b321-0a2af432af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692</td>\n",
       "      <td>2660</td>\n",
       "      <td>2436</td>\n",
       "      <td>1540</td>\n",
       "      <td>2049</td>\n",
       "      <td>4085</td>\n",
       "      <td>4040</td>\n",
       "      <td>3858</td>\n",
       "      <td>3129</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>1919</td>\n",
       "      <td>3565</td>\n",
       "      <td>1958</td>\n",
       "      <td>3721</td>\n",
       "      <td>2581</td>\n",
       "      <td>2120</td>\n",
       "      <td>274</td>\n",
       "      <td>1084</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263</td>\n",
       "      <td>1037</td>\n",
       "      <td>40</td>\n",
       "      <td>145</td>\n",
       "      <td>568</td>\n",
       "      <td>2260</td>\n",
       "      <td>835</td>\n",
       "      <td>3327</td>\n",
       "      <td>1005</td>\n",
       "      <td>4008</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>1381</td>\n",
       "      <td>1413</td>\n",
       "      <td>1544</td>\n",
       "      <td>2068</td>\n",
       "      <td>67</td>\n",
       "      <td>254</td>\n",
       "      <td>1001</td>\n",
       "      <td>3992</td>\n",
       "      <td>3665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>501</td>\n",
       "      <td>1991</td>\n",
       "      <td>3855</td>\n",
       "      <td>3119</td>\n",
       "      <td>175</td>\n",
       "      <td>687</td>\n",
       "      <td>2733</td>\n",
       "      <td>2728</td>\n",
       "      <td>2708</td>\n",
       "      <td>2628</td>\n",
       "      <td>...</td>\n",
       "      <td>3431</td>\n",
       "      <td>1423</td>\n",
       "      <td>1581</td>\n",
       "      <td>2214</td>\n",
       "      <td>652</td>\n",
       "      <td>2596</td>\n",
       "      <td>2177</td>\n",
       "      <td>502</td>\n",
       "      <td>1996</td>\n",
       "      <td>3873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2332</td>\n",
       "      <td>1122</td>\n",
       "      <td>378</td>\n",
       "      <td>1497</td>\n",
       "      <td>1880</td>\n",
       "      <td>3412</td>\n",
       "      <td>1346</td>\n",
       "      <td>1273</td>\n",
       "      <td>981</td>\n",
       "      <td>3911</td>\n",
       "      <td>...</td>\n",
       "      <td>680</td>\n",
       "      <td>2707</td>\n",
       "      <td>2621</td>\n",
       "      <td>2279</td>\n",
       "      <td>909</td>\n",
       "      <td>3623</td>\n",
       "      <td>2189</td>\n",
       "      <td>552</td>\n",
       "      <td>2194</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3623</td>\n",
       "      <td>2189</td>\n",
       "      <td>552</td>\n",
       "      <td>2194</td>\n",
       "      <td>570</td>\n",
       "      <td>2266</td>\n",
       "      <td>860</td>\n",
       "      <td>3426</td>\n",
       "      <td>1403</td>\n",
       "      <td>1503</td>\n",
       "      <td>...</td>\n",
       "      <td>2055</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>147</td>\n",
       "      <td>574</td>\n",
       "      <td>2282</td>\n",
       "      <td>924</td>\n",
       "      <td>3683</td>\n",
       "      <td>2429</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727413</th>\n",
       "      <td>1098</td>\n",
       "      <td>282</td>\n",
       "      <td>1113</td>\n",
       "      <td>341</td>\n",
       "      <td>1351</td>\n",
       "      <td>1296</td>\n",
       "      <td>1074</td>\n",
       "      <td>188</td>\n",
       "      <td>738</td>\n",
       "      <td>2937</td>\n",
       "      <td>...</td>\n",
       "      <td>2328</td>\n",
       "      <td>1106</td>\n",
       "      <td>314</td>\n",
       "      <td>1241</td>\n",
       "      <td>855</td>\n",
       "      <td>3407</td>\n",
       "      <td>1325</td>\n",
       "      <td>1190</td>\n",
       "      <td>652</td>\n",
       "      <td>2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727414</th>\n",
       "      <td>1130</td>\n",
       "      <td>409</td>\n",
       "      <td>1624</td>\n",
       "      <td>2388</td>\n",
       "      <td>1345</td>\n",
       "      <td>1269</td>\n",
       "      <td>965</td>\n",
       "      <td>3848</td>\n",
       "      <td>3092</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>2219</td>\n",
       "      <td>670</td>\n",
       "      <td>2665</td>\n",
       "      <td>2453</td>\n",
       "      <td>1607</td>\n",
       "      <td>2318</td>\n",
       "      <td>1066</td>\n",
       "      <td>154</td>\n",
       "      <td>603</td>\n",
       "      <td>2398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727415</th>\n",
       "      <td>3933</td>\n",
       "      <td>3429</td>\n",
       "      <td>1414</td>\n",
       "      <td>1545</td>\n",
       "      <td>2072</td>\n",
       "      <td>83</td>\n",
       "      <td>317</td>\n",
       "      <td>1254</td>\n",
       "      <td>907</td>\n",
       "      <td>3613</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>501</td>\n",
       "      <td>1989</td>\n",
       "      <td>3846</td>\n",
       "      <td>3083</td>\n",
       "      <td>30</td>\n",
       "      <td>107</td>\n",
       "      <td>414</td>\n",
       "      <td>1643</td>\n",
       "      <td>2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727416</th>\n",
       "      <td>660</td>\n",
       "      <td>2628</td>\n",
       "      <td>2306</td>\n",
       "      <td>1017</td>\n",
       "      <td>4056</td>\n",
       "      <td>3921</td>\n",
       "      <td>3384</td>\n",
       "      <td>1235</td>\n",
       "      <td>831</td>\n",
       "      <td>3310</td>\n",
       "      <td>...</td>\n",
       "      <td>251</td>\n",
       "      <td>989</td>\n",
       "      <td>3941</td>\n",
       "      <td>3464</td>\n",
       "      <td>1554</td>\n",
       "      <td>2105</td>\n",
       "      <td>216</td>\n",
       "      <td>849</td>\n",
       "      <td>3381</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727417</th>\n",
       "      <td>3539</td>\n",
       "      <td>1854</td>\n",
       "      <td>3308</td>\n",
       "      <td>929</td>\n",
       "      <td>3704</td>\n",
       "      <td>2513</td>\n",
       "      <td>1846</td>\n",
       "      <td>3275</td>\n",
       "      <td>797</td>\n",
       "      <td>3175</td>\n",
       "      <td>...</td>\n",
       "      <td>3870</td>\n",
       "      <td>3177</td>\n",
       "      <td>406</td>\n",
       "      <td>1609</td>\n",
       "      <td>2325</td>\n",
       "      <td>1095</td>\n",
       "      <td>271</td>\n",
       "      <td>1069</td>\n",
       "      <td>168</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727418 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9    ...   185  \\\n",
       "0       1692  2660  2436  1540  2049  4085  4040  3858  3129   215  ...   483   \n",
       "1        263  1037    40   145   568  2260   835  3327  1005  4008  ...   349   \n",
       "2        501  1991  3855  3119   175   687  2733  2728  2708  2628  ...  3431   \n",
       "3       2332  1122   378  1497  1880  3412  1346  1273   981  3911  ...   680   \n",
       "4       3623  2189   552  2194   570  2266   860  3426  1403  1503  ...  2055   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "727413  1098   282  1113   341  1351  1296  1074   188   738  2937  ...  2328   \n",
       "727414  1130   409  1624  2388  1345  1269   965  3848  3092    68  ...  2219   \n",
       "727415  3933  3429  1414  1545  2072    83   317  1254   907  3613  ...   129   \n",
       "727416   660  2628  2306  1017  4056  3921  3384  1235   831  3310  ...   251   \n",
       "727417  3539  1854  3308   929  3704  2513  1846  3275   797  3175  ...  3870   \n",
       "\n",
       "         186   187   188   189   190   191   192   193   194  \n",
       "0       1919  3565  1958  3721  2581  2120   274  1084   226  \n",
       "1       1381  1413  1544  2068    67   254  1001  3992  3665  \n",
       "2       1423  1581  2214   652  2596  2177   502  1996  3873  \n",
       "3       2707  2621  2279   909  3623  2189   552  2194   570  \n",
       "4         13    40   147   574  2282   924  3683  2429  1510  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "727413  1106   314  1241   855  3407  1325  1190   652  2595  \n",
       "727414   670  2665  2453  1607  2318  1066   154   603  2398  \n",
       "727415   501  1989  3846  3083    30   107   414  1643  2461  \n",
       "727416   989  3941  3464  1554  2105   216   849  3381  1224  \n",
       "727417  3177   406  1609  2325  1095   271  1069   168   657  \n",
       "\n",
       "[727418 rows x 195 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4b47f7a-7a64-4703-8ae4-299037fadc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/data/projects/Enhancer/New_Finetune_data_with_seqFeature-encodeFeature_raw_data/DNABERT_feature_dataset1.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5fca4d6d-0211-4407-bb86-45a30a54bc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad52510b-8559-45dd-b0e7-b1eb9bc28728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_Fraction</th>\n",
       "      <th>C_Fraction</th>\n",
       "      <th>G_Fraction</th>\n",
       "      <th>T_Fraction</th>\n",
       "      <th>PurPyr_Fraction</th>\n",
       "      <th>AmKe_Fraction</th>\n",
       "      <th>WeSt_Fraction</th>\n",
       "      <th>CpG1</th>\n",
       "      <th>CpG2</th>\n",
       "      <th>CpG3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.368687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.271357</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.211055</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.570707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.328283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.201005</td>\n",
       "      <td>0.328283</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210017</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.211055</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210018</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.180905</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.237374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210019</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.429293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210020</th>\n",
       "      <td>0.310</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.287879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210021</th>\n",
       "      <td>0.270</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.160804</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.328283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210022 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_Fraction  C_Fraction  G_Fraction  T_Fraction  PurPyr_Fraction  \\\n",
       "0            0.185       0.305       0.235       0.275            -0.16   \n",
       "1            0.205       0.315       0.235       0.245            -0.12   \n",
       "2            0.285       0.295       0.315       0.105             0.20   \n",
       "3            0.190       0.265       0.235       0.310            -0.15   \n",
       "4            0.180       0.300       0.240       0.280            -0.16   \n",
       "...            ...         ...         ...         ...              ...   \n",
       "210017       0.185       0.275       0.260       0.280            -0.11   \n",
       "210018       0.245       0.260       0.200       0.295            -0.11   \n",
       "210019       0.185       0.230       0.265       0.320            -0.10   \n",
       "210020       0.310       0.215       0.285       0.190             0.19   \n",
       "210021       0.270       0.220       0.290       0.220             0.12   \n",
       "\n",
       "        AmKe_Fraction  WeSt_Fraction      CpG1      CpG2      CpG3  \n",
       "0               -0.02          -0.08  0.251256  0.348485  0.368687  \n",
       "1                0.04          -0.10  0.271357  0.393939  0.383838  \n",
       "2                0.16          -0.22  0.211055  0.398990  0.570707  \n",
       "3               -0.09           0.00  0.150754  0.262626  0.328283  \n",
       "4               -0.04          -0.08  0.201005  0.328283  0.393939  \n",
       "...               ...            ...       ...       ...       ...  \n",
       "210017          -0.08          -0.07  0.211055  0.348485  0.409091  \n",
       "210018           0.01           0.08  0.180905  0.282828  0.237374  \n",
       "210019          -0.17           0.01  0.140704  0.277778  0.429293  \n",
       "210020           0.05           0.00  0.140704  0.252525  0.287879  \n",
       "210021          -0.02          -0.02  0.160804  0.277778  0.328283  \n",
       "\n",
       "[210022 rows x 10 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdedc746-b413-403a-926c-4499d56ec480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1850,  0.3050,  0.2350,  0.2750, -0.1600, -0.0200, -0.0800,  0.2513,\n",
       "          0.3485,  0.3687]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_tensor = torch.tensor(sequence_data[:1].values)\n",
    "row_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfa6d265-9ca0-4528-a172-1b86f54dcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor = torch.cat((torch.tensor(data[0].input_ids), row_tensor.squeeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd5b26fc-56fd-4313-a4dd-3aebf13dbb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0000e+00,  2.6710e+03,  2.4800e+03,  1.7130e+03,  2.7440e+03,\n",
       "         2.7700e+03,  2.8730e+03,  3.2880e+03,  8.5100e+02,  3.3900e+03,\n",
       "         1.2600e+03,  9.3200e+02,  3.7160e+03,  2.5610e+03,  2.0390e+03,\n",
       "         4.0460e+03,  3.8810e+03,  3.2230e+03,  5.8900e+02,  2.3440e+03,\n",
       "         1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0110e+03,  4.0310e+03,\n",
       "         3.8230e+03,  2.9920e+03,  3.7630e+03,  2.7510e+03,  2.7970e+03,\n",
       "         2.9830e+03,  3.7270e+03,  2.6050e+03,  2.2150e+03,  6.5400e+02,\n",
       "         2.6030e+03,  2.2070e+03,  6.2300e+02,  2.4770e+03,  1.7040e+03,\n",
       "         2.7070e+03,  2.6220e+03,  2.2810e+03,  9.1700e+02,  3.6540e+03,\n",
       "         2.3140e+03,  1.0500e+03,  9.0000e+01,  3.4600e+02,  1.3700e+03,\n",
       "         1.3700e+03,  1.3720e+03,  1.3780e+03,  1.4010e+03,  1.4940e+03,\n",
       "         1.8660e+03,  3.3540e+03,  1.1140e+03,  3.4600e+02,  1.3700e+03,\n",
       "         1.3690e+03,  1.3680e+03,  1.3620e+03,  1.3370e+03,  1.2400e+03,\n",
       "         8.4900e+02,  3.3840e+03,  1.2330e+03,  8.2200e+02,  3.2760e+03,\n",
       "         8.0400e+02,  3.2040e+03,  5.1600e+02,  2.0500e+03,  4.0900e+03,\n",
       "         4.0580e+03,  3.9310e+03,  3.4210e+03,  1.3830e+03,  1.4230e+03,\n",
       "         1.5810e+03,  2.2140e+03,  6.5200e+02,  2.5940e+03,  2.1700e+03,\n",
       "         4.7300e+02,  1.8800e+03,  3.4110e+03,  1.3430e+03,  1.2610e+03,\n",
       "         9.3600e+02,  3.7320e+03,  2.6250e+03,  2.2940e+03,  9.7200e+02,\n",
       "         3.8760e+03,  3.2020e+03,  5.0700e+02,  2.0140e+03,  3.9470e+03,\n",
       "         3.4880e+03,  1.6490e+03,  2.4860e+03,  1.7390e+03,  2.8460e+03,\n",
       "         3.1790e+03,  4.1500e+02,  1.6460e+03,  2.4760e+03,  1.6970e+03,\n",
       "         2.6790e+03,  2.5110e+03,  1.8380e+03,  3.2430e+03,  6.7200e+02,\n",
       "         2.6740e+03,  2.4920e+03,  1.7610e+03,  2.9340e+03,  3.5310e+03,\n",
       "         1.8220e+03,  3.1800e+03,  4.1900e+02,  1.6630e+03,  2.5420e+03,\n",
       "         1.9640e+03,  3.7470e+03,  2.6870e+03,  2.5420e+03,  1.9630e+03,\n",
       "         3.7410e+03,  2.6640e+03,  2.4510e+03,  1.5990e+03,  2.2860e+03,\n",
       "         9.3900e+02,  3.7430e+03,  2.6710e+03,  2.4770e+03,  1.7010e+03,\n",
       "         2.6930e+03,  2.5680e+03,  2.0660e+03,  6.0000e+01,  2.2700e+02,\n",
       "         8.9400e+02,  3.5640e+03,  1.9560e+03,  3.7160e+03,  2.5610e+03,\n",
       "         2.0380e+03,  4.0420e+03,  3.8650e+03,  3.1590e+03,  3.3300e+02,\n",
       "         1.3200e+03,  1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0100e+03,\n",
       "         4.0280e+03,  3.8090e+03,  2.9360e+03,  3.5390e+03,  1.8550e+03,\n",
       "         3.3090e+03,  9.3500e+02,  3.7270e+03,  2.6080e+03,  2.2270e+03,\n",
       "         7.0400e+02,  2.8030e+03,  3.0070e+03,  3.8230e+03,  2.9920e+03,\n",
       "         3.7640e+03,  2.7550e+03,  2.8150e+03,  3.0550e+03,  4.0140e+03,\n",
       "         3.7530e+03,  2.7090e+03,  2.6300e+03,  2.3130e+03,  1.0470e+03,\n",
       "         7.7000e+01,  2.9600e+02,  1.1710e+03,  5.7400e+02,  2.2810e+03,\n",
       "         9.1800e+02,  3.6580e+03,  2.3300e+03,  1.1140e+03,  3.4600e+02,\n",
       "         1.3700e+03,  3.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         1.8500e-01,  3.0500e-01,  2.3500e-01,  2.7500e-01, -1.6000e-01,\n",
       "        -2.0000e-02, -8.0000e-02,  2.5126e-01,  3.4848e-01,  3.6869e-01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "845db65f-b223-41d8-8dc5-8c3d953cbf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33e6059f-b659-4c2f-8718-6f13e6ed8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tensor_existing\n",
    "split_point = len(torch.tensor(data[0].input_ids)) - 5\n",
    "first_part, second_part = torch.tensor(data[0].input_ids[:split_point]), torch.tensor(data[0].input_ids[split_point:])\n",
    "\n",
    "# Concatenate the tensors\n",
    "new_concatenated_tensor = torch.cat([first_part, row_tensor.squeeze(0), second_part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "856078b3-3358-4a06-b99a-c20e91c86946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0000e+00,  2.6710e+03,  2.4800e+03,  1.7130e+03,  2.7440e+03,\n",
       "         2.7700e+03,  2.8730e+03,  3.2880e+03,  8.5100e+02,  3.3900e+03,\n",
       "         1.2600e+03,  9.3200e+02,  3.7160e+03,  2.5610e+03,  2.0390e+03,\n",
       "         4.0460e+03,  3.8810e+03,  3.2230e+03,  5.8900e+02,  2.3440e+03,\n",
       "         1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0110e+03,  4.0310e+03,\n",
       "         3.8230e+03,  2.9920e+03,  3.7630e+03,  2.7510e+03,  2.7970e+03,\n",
       "         2.9830e+03,  3.7270e+03,  2.6050e+03,  2.2150e+03,  6.5400e+02,\n",
       "         2.6030e+03,  2.2070e+03,  6.2300e+02,  2.4770e+03,  1.7040e+03,\n",
       "         2.7070e+03,  2.6220e+03,  2.2810e+03,  9.1700e+02,  3.6540e+03,\n",
       "         2.3140e+03,  1.0500e+03,  9.0000e+01,  3.4600e+02,  1.3700e+03,\n",
       "         1.3700e+03,  1.3720e+03,  1.3780e+03,  1.4010e+03,  1.4940e+03,\n",
       "         1.8660e+03,  3.3540e+03,  1.1140e+03,  3.4600e+02,  1.3700e+03,\n",
       "         1.3690e+03,  1.3680e+03,  1.3620e+03,  1.3370e+03,  1.2400e+03,\n",
       "         8.4900e+02,  3.3840e+03,  1.2330e+03,  8.2200e+02,  3.2760e+03,\n",
       "         8.0400e+02,  3.2040e+03,  5.1600e+02,  2.0500e+03,  4.0900e+03,\n",
       "         4.0580e+03,  3.9310e+03,  3.4210e+03,  1.3830e+03,  1.4230e+03,\n",
       "         1.5810e+03,  2.2140e+03,  6.5200e+02,  2.5940e+03,  2.1700e+03,\n",
       "         4.7300e+02,  1.8800e+03,  3.4110e+03,  1.3430e+03,  1.2610e+03,\n",
       "         9.3600e+02,  3.7320e+03,  2.6250e+03,  2.2940e+03,  9.7200e+02,\n",
       "         3.8760e+03,  3.2020e+03,  5.0700e+02,  2.0140e+03,  3.9470e+03,\n",
       "         3.4880e+03,  1.6490e+03,  2.4860e+03,  1.7390e+03,  2.8460e+03,\n",
       "         3.1790e+03,  4.1500e+02,  1.6460e+03,  2.4760e+03,  1.6970e+03,\n",
       "         2.6790e+03,  2.5110e+03,  1.8380e+03,  3.2430e+03,  6.7200e+02,\n",
       "         2.6740e+03,  2.4920e+03,  1.7610e+03,  2.9340e+03,  3.5310e+03,\n",
       "         1.8220e+03,  3.1800e+03,  4.1900e+02,  1.6630e+03,  2.5420e+03,\n",
       "         1.9640e+03,  3.7470e+03,  2.6870e+03,  2.5420e+03,  1.9630e+03,\n",
       "         3.7410e+03,  2.6640e+03,  2.4510e+03,  1.5990e+03,  2.2860e+03,\n",
       "         9.3900e+02,  3.7430e+03,  2.6710e+03,  2.4770e+03,  1.7010e+03,\n",
       "         2.6930e+03,  2.5680e+03,  2.0660e+03,  6.0000e+01,  2.2700e+02,\n",
       "         8.9400e+02,  3.5640e+03,  1.9560e+03,  3.7160e+03,  2.5610e+03,\n",
       "         2.0380e+03,  4.0420e+03,  3.8650e+03,  3.1590e+03,  3.3300e+02,\n",
       "         1.3200e+03,  1.1720e+03,  5.7900e+02,  2.3040e+03,  1.0100e+03,\n",
       "         4.0280e+03,  3.8090e+03,  2.9360e+03,  3.5390e+03,  1.8550e+03,\n",
       "         3.3090e+03,  9.3500e+02,  3.7270e+03,  2.6080e+03,  2.2270e+03,\n",
       "         7.0400e+02,  2.8030e+03,  3.0070e+03,  3.8230e+03,  2.9920e+03,\n",
       "         3.7640e+03,  2.7550e+03,  2.8150e+03,  3.0550e+03,  4.0140e+03,\n",
       "         3.7530e+03,  2.7090e+03,  2.6300e+03,  2.3130e+03,  1.0470e+03,\n",
       "         7.7000e+01,  2.9600e+02,  1.1710e+03,  5.7400e+02,  2.2810e+03,\n",
       "         9.1800e+02,  3.6580e+03,  2.3300e+03,  1.1140e+03,  3.4600e+02,\n",
       "         1.8500e-01,  3.0500e-01,  2.3500e-01,  2.7500e-01, -1.6000e-01,\n",
       "        -2.0000e-02, -8.0000e-02,  2.5126e-01,  3.4848e-01,  3.6869e-01,\n",
       "         1.3700e+03,  3.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_concatenated_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8827964-6fb4-44dd-b42f-2413632688ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 2671, 2480, 1713, 2744, 2770, 2873, 3288,  851, 3390, 1260,  932,\n",
       "        3716, 2561, 2039, 4046, 3881, 3223,  589, 2344, 1172,  579, 2304, 1011,\n",
       "        4031, 3823, 2992, 3763, 2751, 2797, 2983, 3727, 2605, 2215,  654, 2603,\n",
       "        2207,  623, 2477, 1704, 2707, 2622, 2281,  917, 3654, 2314, 1050,   90,\n",
       "         346, 1370, 1370, 1372, 1378, 1401, 1494, 1866, 3354, 1114,  346, 1370,\n",
       "        1369, 1368, 1362, 1337, 1240,  849, 3384, 1233,  822, 3276,  804, 3204,\n",
       "         516, 2050, 4090, 4058, 3931, 3421, 1383, 1423, 1581, 2214,  652, 2594,\n",
       "        2170,  473, 1880, 3411, 1343, 1261,  936, 3732, 2625, 2294,  972, 3876,\n",
       "        3202,  507, 2014, 3947, 3488, 1649, 2486, 1739, 2846, 3179,  415, 1646,\n",
       "        2476, 1697, 2679, 2511, 1838, 3243,  672, 2674, 2492, 1761, 2934, 3531,\n",
       "        1822, 3180,  419, 1663, 2542, 1964, 3747, 2687, 2542, 1963, 3741, 2664,\n",
       "        2451, 1599, 2286,  939, 3743, 2671, 2477, 1701, 2693, 2568, 2066,   60,\n",
       "         227,  894, 3564, 1956, 3716, 2561, 2038, 4042, 3865, 3159,  333, 1320,\n",
       "        1172,  579, 2304, 1010, 4028, 3809, 2936, 3539, 1855, 3309,  935, 3727,\n",
       "        2608, 2227,  704, 2803, 3007, 3823, 2992, 3764, 2755, 2815, 3055, 4014,\n",
       "        3753, 2709, 2630, 2313, 1047,   77,  296, 1171,  574, 2281,  918, 3658,\n",
       "        2330, 1114,  346, 1370,    3,    0,    0,    0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data[0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c2d8f46-fad4-415e-8efe-752465235d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1850,  0.3050,  0.2350,  0.2750, -0.1600, -0.0200, -0.0800,  0.2513,\n",
       "         0.3485,  0.3687], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_tensor.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "344a7004-af4e-42b9-a77a-b5f942868f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 2671, 2480, 1713, 2744, 2770, 2873, 3288,  851, 3390, 1260,  932,\n",
       "        3716, 2561, 2039, 4046, 3881, 3223,  589, 2344, 1172,  579, 2304, 1011,\n",
       "        4031, 3823, 2992, 3763, 2751, 2797, 2983, 3727, 2605, 2215,  654, 2603,\n",
       "        2207,  623, 2477, 1704, 2707, 2622, 2281,  917, 3654, 2314, 1050,   90,\n",
       "         346, 1370, 1370, 1372, 1378, 1401, 1494, 1866, 3354, 1114,  346, 1370,\n",
       "        1369, 1368, 1362, 1337, 1240,  849, 3384, 1233,  822, 3276,  804, 3204,\n",
       "         516, 2050, 4090, 4058, 3931, 3421, 1383, 1423, 1581, 2214,  652, 2594,\n",
       "        2170,  473, 1880, 3411, 1343, 1261,  936, 3732, 2625, 2294,  972, 3876,\n",
       "        3202,  507, 2014, 3947, 3488, 1649, 2486, 1739, 2846, 3179,  415, 1646,\n",
       "        2476, 1697, 2679, 2511, 1838, 3243,  672, 2674, 2492, 1761, 2934, 3531,\n",
       "        1822, 3180,  419, 1663, 2542, 1964, 3747, 2687, 2542, 1963, 3741, 2664,\n",
       "        2451, 1599, 2286,  939, 3743, 2671, 2477, 1701, 2693, 2568, 2066,   60,\n",
       "         227,  894, 3564, 1956, 3716, 2561, 2038, 4042, 3865, 3159,  333, 1320,\n",
       "        1172,  579, 2304, 1010, 4028, 3809, 2936, 3539, 1855, 3309,  935, 3727,\n",
       "        2608, 2227,  704, 2803, 3007, 3823, 2992, 3764, 2755, 2815, 3055, 4014,\n",
       "        3753, 2709, 2630, 2313, 1047,   77,  296, 1171,  574, 2281,  918, 3658,\n",
       "        2330, 1114,  346, 1370,    3,    0,    0,    0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data[0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "402dfa47-99a2-426e-b8a4-4142f51cc594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 220])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor1 = torch.zeros((1000, 210))\n",
    "tensor1[:, -14] = 3\n",
    "\n",
    "tensor2 = torch.rand((1000, 10))  # Some random values just for the sake of the example\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate tensors\n",
    "result = torch.cat([tensor1[:, :-14], tensor2, tensor1[:, -14:]], dim=1)\n",
    "\n",
    "print(result.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2066c4e1-8324-4b2f-9635-36f3d8f45279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0417, 0.0379,\n",
       "        0.0866, 0.5039, 0.7885, 0.7678, 0.0440, 0.3612, 0.0651, 0.4394, 3.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b11a0c-1580-4c5b-b3ec-7cbdc65da640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
